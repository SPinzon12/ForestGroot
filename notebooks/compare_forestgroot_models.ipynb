{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 🌳 Comparación de Modelos: ForestGroot YOLOv8 vs. YOLOv9\n",
        "\n",
        "## Integrantes:\n",
        "- **Juan Camilo Vargas**\n",
        "- **Samuel Pinzón Valderrutén**\n",
        "- **Sebastian Diaz Noguera**\n",
        "\n",
        "![ForestGroot](https://i.ibb.co/WzNPDXN/Sin-t-tulo.png)"
      ],
      "metadata": {
        "id": "1fEYTArp6zmX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dataset Forestgroot**\n",
        "\n",
        "Para la creación de este dataset se recolectaron un total de 400 imágenes JPG de zonas adyacentes al Amazonas utilizando imágenes de [Google Earth](https://www.google.com/earth/). A continuación, se presenta una tabla que describe las clases identificadas en las imágenes recolectadas:\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "| **Clase**       | **Descripción**                                        |\n",
        "|-----------------|--------------------------------------------------------|\n",
        "| deforestation   | Áreas donde se observa la pérdida de bosques.          |\n",
        "| water           | Regiones que contienen ríos y cuerpos de agua.         |\n",
        "| fire            | Zonas afectadas por incendios forestales.              |\n",
        "| urban           | Áreas urbanas, incluyendo ciudades y asentamientos urbanos. |\n",
        "\n",
        "</div>\n",
        "\n",
        "Debido al desbalance de clase con las 400 imágenes se realizó un data augmentation en las siguientes clases para evitar este problema:\n",
        "\n",
        "- **urban:** 245 imágenes.\n",
        "- **fire:** 85 imágenes.\n",
        "- **water:** 100 imágenes.\n",
        "\n",
        "Se obtuvo un dataset con un total de **830** imágenes, donde se identificaron las siguientes cantidades después del etiquetado:\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "| Clase          | Cantidad |\n",
        "|----------------|----------|\n",
        "| deforestation  | 558      |\n",
        "| urban          | 511      |\n",
        "| fire           | 477      |\n",
        "| water          | 470      |\n",
        "\n",
        "</div>\n",
        "\n",
        "Posteriormente, se realizó el data augmentation con Roboflow y se obtuvo el dataset final con **1992** imágenes, distribuidas de la siguiente manera:\n",
        "\n",
        "- Conjunto de entrenamiento: **1743** imágenes.\n",
        "- Conjunto de prueba: **166** imágenes.\n",
        "- Conjunto de validación: **83** imágenes.\n",
        "\n",
        "**Ejemplo del Dataset:**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.ibb.co/nrfJFSW/deforestation-098.png\" alt=\"ForestGroot\" width=\"70%\">\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "IdM4Tg9JXJNn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**YOLOv8**\n",
        "YOLOv8 es una versión de la familia de algoritmos de detección de objetos conocida como You Only Look Once (YOLO). Este modelo destaca por su capacidad para equilibrar velocidad y precisión en tareas de visión artificial en tiempo real. YOLOv8 sigue este enfoque al no contar con capas densas, todas sus capas son convolucionales. Esto lo hace adecuado para escenarios donde el procesamiento en tiempo real es crucial.\n",
        "\n",
        "**Proceso de detección de objetos en YOLOv8:**\n",
        "\n",
        "1. **Extracción de características:**\n",
        "   - La imagen se divide en una cuadrícula de celdas, y luego se utiliza una red neuronal convolucional para extraer características de cada una.\n",
        "   - Estas características representan la información visual dentro de cada celda.\n",
        "\n",
        "2. **Predicción de Cajas Delimitadoras y Clases:**\n",
        "   - Para cada celda de la cuadrícula, YOLOv8 predice un número fijo de cajas delimitadoras y las probabilidades de las clases correspondientes.\n",
        "   - Cada caja delimitadora contiene información sobre la posición (coordenadas x, y, ancho y alto) y la confianza de detección.\n",
        "\n",
        "3. **Supresión de No Máximos (NMS):**\n",
        "   - Se aplica NMS para eliminar detecciones redundantes y mantener solo las detecciones más confiables.\n",
        "   - Durante este proceso, se selecciona la caja delimitadora con la mayor probabilidad de objeto y se eliminan las demás cajas que tienen una superposición significativa con ella.\n",
        "\n",
        "4. **Salida de Detecciones:**\n",
        "   - La salida final consiste en las cajas delimitadoras seleccionadas después de la supresión de no máximos, junto con las clases detectadas y sus respectivas probabilidades.\n",
        "\n",
        "**Arquitectura:**\n",
        "\n",
        "![yolov8_arciteture](https://user-images.githubusercontent.com/62583018/211719362-39fc8a88-b1ce-4ab3-9a9f-b640550515b4.jpg)"
      ],
      "metadata": {
        "id": "JRz0LNQETMvm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Roboflow\n",
        "\n",
        "**[Roboflow](https://roboflow.com/)** es una plataforma que facilita el proceso de preparación de datos para proyectos de visión por computadora y aprendizaje automático. Proporciona herramientas para organizar, limpiar, etiquetar y aumentar conjuntos de datos de imágenes de forma eficiente. Además, facilita la creación de modelos de aprendizaje automático a través de su plataforma.\n",
        "\n",
        "La preparación de un conjunto de datos personalizado puede representar un desafío considerable, pero como se mencionó, Roboflow simplifica enormemente este proceso. A continuación, se describen los pasos seguidos para llevar a cabo este procedimiento:\n",
        "\n",
        "1. **Creación del Proyecto:** Se inició un proyecto específico para la detección de objetos en Roboflow.\n",
        "2. **Subida de Imágenes:** Se subieron las imágenes recolectadas previamente al proyecto creado.\n",
        "3. **Etiquetado de Clases:** Se realizó el etiquetado de las clases de interés en las imágenes, como deforestación, agua, fuego y urbano.\n",
        "4. **Aumento de Datos:** Se aplicaron técnicas de aumento de datos para mejorar la diversidad y cantidad de imágenes disponibles.\n",
        "5. **Exportación del Dataset:** Finalmente, se exportó el dataset preparado en el formato adecuado para su uso en el entorno de entrenamiento.\n",
        "\n",
        "[![Roboflow](https://i.ibb.co/kMrgGYL/Sin-t-tulo.png)](https://ibb.co/CWX5wxj)\n",
        "\n"
      ],
      "metadata": {
        "id": "pBd1GXdnaT2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Detección de Deforestación**"
      ],
      "metadata": {
        "id": "zT60EtBuk5wI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Descarga de las Librerías**"
      ],
      "metadata": {
        "id": "ShjZ3utQc5hV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iw2EMFzahSl0",
        "outputId": "15033730-a845-46d9-a29e-183b5d5f259c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.6/779.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.5/75.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.3/158.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Instalación de paquetes necesarios\n",
        "!pip install ultralytics -q\n",
        "!pip install roboflow -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importación de librerías\n",
        "import glob\n",
        "import os\n",
        "\n",
        "from IPython.display import display, Image\n",
        "from roboflow import Roboflow\n",
        "import tensorflow as tf\n",
        "from ultralytics import YOLO"
      ],
      "metadata": {
        "id": "f9fcIwGsixKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Entorno de Trabajo**\n",
        "\n",
        "Es importante asegurarse de contar con acceso a una unidad de procesamiento gráfico (GPU) para garantizar un rendimiento óptimo durante el proceso."
      ],
      "metadata": {
        "id": "8HEQQkLderbV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9EcCFyYltYQ",
        "outputId": "8a377038-c700-428e-a9b4-7062988d998f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dispositivo GPU encontrado en: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "# Verificar si se encuentra disponible la GPU\n",
        "device_name = tf.test.gpu_device_name()\n",
        "# Si no se encuentra disponible la GPU, lanzar una excepción\n",
        "if device_name != '/device:GPU:0':\n",
        "    raise SystemError('No se encontró dispositivo GPU')\n",
        "# Imprimir el nombre del dispositivo GPU encontrado\n",
        "print('Dispositivo GPU encontrado en: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define el directorio de trabajo actual\n",
        "HOME = os.getcwd()\n",
        "HOME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xYtmxxN9i6xe",
        "outputId": "29300a6e-98b3-4c2f-ab64-a3a472766e2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset desde Roboflow**\n",
        "\n",
        "Como se mencionó anteriormente, se utilizó la herramienta de Roboflow para la creación del dataset. Para acceder al dataset, se puede utilizar la API de la siguiente manera. En caso de que no puedas acceder a la API, también puedes descomprimir el archivo con el dataset que se encuentra en el repositorio."
      ],
      "metadata": {
        "id": "GCCgv6n7e6E4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir {HOME}/datasets\n",
        "%cd {HOME}/datasets\n",
        "\n",
        "# Descargar el conjunto de datos de Roboflow utilizando la API de Roboflow\n",
        "rf = Roboflow(api_key=\"w2EZPylLkOYJSnI4o6qN\")\n",
        "project = rf.workspace(\"test-yyciu\").project(\"groot-v2\")\n",
        "version = project.version(3)\n",
        "dataset = version.download(\"yolov8\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-gXImqhntGR",
        "outputId": "50818887-d739-4e59-9a99-234804634dcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/datasets\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Dependency ultralytics==8.0.196 is required but found version=8.2.28, to fix: `pip install ultralytics==8.0.196`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in Groot-V2-3 to yolov8:: 100%|██████████| 139468/139468 [00:04<00:00, 30198.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Groot-V2-3 in yolov8:: 100%|██████████| 3996/3996 [00:00<00:00, 4300.30it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validación**"
      ],
      "metadata": {
        "id": "DDNhOMa5g6md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "# Validar los modelos\n",
        "models_path = f'{HOME}/models'\n",
        "\n",
        "for forestgroot_model in os.listdir(models_path):\n",
        "    if not forestgroot_model.lower().endswith('.pt'):\n",
        "      continue\n",
        "    model_name = ' '.join(forestgroot_model.split('_')[:2])\n",
        "    print(f\"****** Model: {model_name} ******\")\n",
        "    model_path = os.path.join(models_path, forestgroot_model)\n",
        "    forestgroot_seg = YOLO(model_path)\n",
        "    validation_results = forestgroot_seg.val()\n",
        "    print(\"****************************************\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8BMx5jti-rW",
        "outputId": "818c7733-a3cb-4242-d7db-fde4ddd1328b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "****** Model: forestgroot yolov8s ******\n",
            "Ultralytics YOLOv8.2.28 🚀 Python-3.10.12 torch-2.3.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "YOLOv8s-seg summary (fused): 195 layers, 11781148 parameters, 0 gradients, 42.4 GFLOPs\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 755k/755k [00:00<00:00, 23.8MB/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/Groot-V2-3/valid/labels... 166 images, 1 backgrounds, 0 corrupt: 100%|██████████| 166/166 [00:00<00:00, 1343.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/Groot-V2-3/valid/labels.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:12<00:00,  1.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        166        415       0.75      0.718      0.769      0.611       0.73      0.731      0.765      0.573\n",
            "         deforestation         37        106      0.826      0.623      0.763      0.679      0.784      0.632       0.75      0.638\n",
            "                  fire         39        100      0.695      0.773      0.789      0.665       0.69        0.8      0.801      0.632\n",
            "                 urban         68         93      0.703       0.71      0.758      0.474      0.685       0.72      0.741      0.417\n",
            "                 water         75        116      0.775      0.767      0.767      0.627      0.762      0.773      0.768      0.604\n",
            "Speed: 9.1ms preprocess, 13.1ms inference, 0.0ms loss, 9.6ms postprocess per image\n",
            "Results saved to \u001b[1mruns/segment/val2\u001b[0m\n",
            "****************************************\n",
            "\n",
            "****** Model: forestgroot yolov9c ******\n",
            "WARNING ⚠️ /content/models/forestgroot_yolov9c_seg.pt appears to require 'dill', which is not in ultralytics requirements.\n",
            "AutoInstall will run now for 'dill' but this feature will be removed in the future.\n",
            "Recommend fixes are to train a new model using the latest 'ultralytics' package or to run a command with an official YOLOv8 model, i.e. 'yolo predict model=yolov8n.pt'\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['dill'] not found, attempting AutoUpdate...\n",
            "Collecting dill\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 116.3/116.3 kB 3.6 MB/s eta 0:00:00\n",
            "Installing collected packages: dill\n",
            "Successfully installed dill-0.3.8\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 6.4s, installed 1 package: ['dill']\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "Ultralytics YOLOv8.2.28 🚀 Python-3.10.12 torch-2.3.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "YOLOv9c-seg summary (fused): 411 layers, 27627612 parameters, 0 gradients, 157.6 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/Groot-V2-3/valid/labels.cache... 166 images, 1 backgrounds, 0 corrupt: 100%|██████████| 166/166 [00:00<?, ?it/s]\n",
            "os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:11<00:00,  1.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        166        415      0.768      0.755      0.782      0.639      0.772      0.754      0.785       0.58\n",
            "         deforestation         37        106      0.762      0.689       0.75      0.676      0.758      0.679      0.733      0.619\n",
            "                  fire         39        100      0.717       0.77      0.796      0.675      0.732       0.78      0.808      0.626\n",
            "                 urban         68         93      0.747      0.763      0.776      0.515      0.761      0.774      0.797      0.448\n",
            "                 water         75        116      0.845      0.798      0.805       0.69      0.839      0.784      0.805      0.626\n",
            "Speed: 3.2ms preprocess, 36.6ms inference, 0.0ms loss, 3.4ms postprocess per image\n",
            "Results saved to \u001b[1mruns/segment/val3\u001b[0m\n",
            "****************************************\n",
            "\n",
            "****** Model: forestgroot yolov8n ******\n",
            "Ultralytics YOLOv8.2.28 🚀 Python-3.10.12 torch-2.3.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "YOLOv8n-seg summary (fused): 195 layers, 3258844 parameters, 0 gradients, 12.0 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/Groot-V2-3/valid/labels.cache... 166 images, 1 backgrounds, 0 corrupt: 100%|██████████| 166/166 [00:00<?, ?it/s]\n",
            "os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:11<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        166        415      0.785      0.703      0.771      0.609      0.789      0.708      0.775      0.573\n",
            "         deforestation         37        106      0.766      0.698      0.742      0.659      0.754      0.689       0.74      0.616\n",
            "                  fire         39        100      0.805        0.7        0.8      0.675      0.816      0.712      0.807      0.645\n",
            "                 urban         68         93      0.747      0.688      0.762      0.492      0.756      0.701      0.771      0.449\n",
            "                 water         75        116      0.823      0.724       0.78      0.611      0.831      0.733      0.782      0.579\n",
            "Speed: 4.6ms preprocess, 10.7ms inference, 0.0ms loss, 5.8ms postprocess per image\n",
            "Results saved to \u001b[1mruns/segment/val4\u001b[0m\n",
            "****************************************\n",
            "\n",
            "****** Model: forestgroot yolov8m ******\n",
            "Ultralytics YOLOv8.2.28 🚀 Python-3.10.12 torch-2.3.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "YOLOv8m-seg summary (fused): 245 layers, 27224700 parameters, 0 gradients, 110.0 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/Groot-V2-3/valid/labels.cache... 166 images, 1 backgrounds, 0 corrupt: 100%|██████████| 166/166 [00:00<?, ?it/s]\n",
            "os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:11<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        166        415      0.724      0.736      0.773      0.627      0.735      0.747      0.775      0.576\n",
            "         deforestation         37        106      0.736      0.689      0.751      0.689      0.737      0.689      0.749      0.625\n",
            "                  fire         39        100       0.66      0.737      0.774      0.631      0.659      0.735      0.773       0.61\n",
            "                 urban         68         93      0.717      0.753      0.759      0.494      0.758      0.796      0.776       0.45\n",
            "                 water         75        116      0.783      0.767      0.806      0.693      0.783      0.767      0.803      0.619\n",
            "Speed: 4.5ms preprocess, 28.2ms inference, 0.2ms loss, 3.7ms postprocess per image\n",
            "Results saved to \u001b[1mruns/segment/val5\u001b[0m\n",
            "****************************************\n",
            "\n",
            "****** Model: forestgroot yolov8l ******\n",
            "Ultralytics YOLOv8.2.28 🚀 Python-3.10.12 torch-2.3.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "YOLOv8l-seg summary (fused): 295 layers, 45914972 parameters, 0 gradients, 220.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/Groot-V2-3/valid/labels.cache... 166 images, 1 backgrounds, 0 corrupt: 100%|██████████| 166/166 [00:00<?, ?it/s]\n",
            "os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:12<00:00,  1.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        166        415      0.786      0.729      0.784      0.642      0.782      0.725       0.78      0.585\n",
            "         deforestation         37        106      0.803      0.693      0.765      0.699      0.803      0.693      0.766      0.645\n",
            "                  fire         39        100      0.745      0.758      0.805       0.67      0.745      0.759      0.804      0.643\n",
            "                 urban         68         93      0.766      0.656      0.758      0.504      0.765      0.656      0.761      0.428\n",
            "                 water         75        116      0.831       0.81      0.807      0.694      0.813      0.793      0.788      0.625\n",
            "Speed: 3.3ms preprocess, 43.4ms inference, 0.0ms loss, 3.4ms postprocess per image\n",
            "Results saved to \u001b[1mruns/segment/val6\u001b[0m\n",
            "****************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultados del Modelo ForestGroot:**\n",
        "<div align=\"center\">\n",
        "\n",
        "| Modelo               | mAP50 <sub>box</sub> deforestation | mAP50<sub>mask</sub> deforestation | Params (M) |\n",
        "|----------------------|:----------------------------------:|:---------------------------------:|:----------:|\n",
        "| ForestGroot Yolov8n  |                0.742               |               0.74                |    3.26    |\n",
        "| ForestGroot Yolov8s  |               **0.763**            |               0.75                |   11.78    |\n",
        "| ForestGroot Yolov8m  |                0.751               |               0.749               |   27.22    |\n",
        "| ForestGroot Yolov8l  |               **0.765**            |             **0.766**             |   45.91    |\n",
        "| ForestGroot Yolov9c  |                0.75                |               0.73                |   27.63    |\n",
        "\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "GxWDawnHHGLt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Elección del Modelo Final\n",
        "\n",
        "Para la elección del modelo final, después de realizar la validación de los modelos, tuvimos en cuenta principalmente la métrica **mAP50** (mean Average Precision at IoU=0.50). Los modelos que mejor rendimiento obtuvieron fueron el **ForestGroot YOLOv8L** y el **ForestGroot YOLOv9c**, ambos con un rendimiento de **0.78** en todas las clases, tanto en la detección de boxes como en máscaras (masks).\n",
        "\n",
        "Sin embargo, dado que nuestro principal objetivo es la detección de la clase **deforestation**, la decisión final se basó en el rendimiento específico en esta clase. En este aspecto, el modelo **ForestGroot YOLOv8L** tuvo un rendimiento superior, alcanzando un **0.76** tanto en la detección de boxes como en máscaras. En comparación, el **ForestGroot YOLOv9c** obtuvo un **0.75** en boxes y un **0.73** en máscaras para la clase **deforestation**.\n",
        "\n",
        "Debido a esta diferencia en la clase más importante para nuestro proyecto, decidimos que el modelo **ForestGroot YOLOv8L** es el más adecuado para nuestro objetivo.\n"
      ],
      "metadata": {
        "id": "gVwvZyuShnj9"
      }
    }
  ]
}